"""
RAG Evaluation Module

This module contains various evaluation methods for assessing the quality
of answers generated by Retrieval-Augmented Generation (RAG) systems.
"""

from typing import Dict, List, Any, Optional
import re
from dataclasses import dataclass


@dataclass
class EvaluationResult:
    """Standardized result structure for all evaluations"""
    score: float  # 0-1 scale
    reasoning: str
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class RAGEvaluationInput:
    """Input structure for RAG evaluation"""
    question: str
    answer: str
    context: str
    retrieved_docs: List[Dict[str, Any]] = None
    ground_truth: Optional[str] = None

    def __post_init__(self):
        if self.retrieved_docs is None:
            self.retrieved_docs = []


class BaseEvaluator:
    """Base class for all RAG evaluators"""

    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description

    def evaluate(self, input_data: RAGEvaluationInput) -> EvaluationResult:
        """Evaluate the RAG output. Must be implemented by subclasses."""
        raise NotImplementedError("Subclasses must implement evaluate method")

    def _preprocess_text(self, text: str) -> str:
        """Basic text preprocessing"""
        if not text:
            return ""

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text.strip())

        # Remove special characters that might interfere with evaluation
        text = re.sub(r'[^\w\s\u00C0-\u017F.,!?;:()\-\'\"]', '', text)

        return text.lower()

    def _calculate_confidence_interval(self, score: float, sample_size: int) -> Dict[str, float]:
        """Calculate confidence interval for the score"""
        # Simplified confidence interval calculation
        margin = 1.96 * (0.5 / (sample_size ** 0.5)) if sample_size > 0 else 0
        return {
            "lower_bound": max(0, score - margin),
            "upper_bound": min(1, score + margin)
        }


def create_evaluation_input(
    question: str,
    answer: str,
    context: str,
    retrieved_docs: List[Dict[str, Any]] = None,
    ground_truth: str = None
) -> RAGEvaluationInput:
    """Helper function to create evaluation input"""
    return RAGEvaluationInput(
        question=question,
        answer=answer,
        context=context,
        retrieved_docs=retrieved_docs or [],
        ground_truth=ground_truth
    )
